import pandas as pd
import numpy as np
text = pd.read_csv('C:\\Users\\----\\Desktop\\python\\ML\\Proposal\\healthdata.csv', 
                   engine='python')

### 1.1 number of words
text['word_count'] = text['content'].apply(lambda x: len(str(x).split(" ")))

text[['content','word_count']].head()

### 1.2 Number of characters
text['char_count'] = text['content'].str.len() ## this also includes spaces
text[['content','char_count']].head()

### 1.3 Average word length
def avg_word(sentence):
    words = sentence.split()
    return (sum(len(word) for word in words)/len(words))

text['avg_word'] = text['content'].apply(lambda x: avg_word(x))

text[['content','avg_word']].head()


from nltk.corpus import stopwords 
stop = stopwords.words('english')

text['stopwords'] = text['content'].apply(lambda x: len([x for x in x.split() if x in stop]))
text[['content','stopwords']].head()

### 1.5 Number of sepcial characters
text['hashtags'] = text['content'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))
text[['content','hashtags']].head()

## 1.6 Number of numeric
text['numerics'] = text['content'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
text[['content','numerics']].head()

text['content'] = text['content'].str.replace('[^\w\s]','')
text['content'].head()


### 2.3 Removing of Stop Words
from nltk.corpus import stopwords
stop = stopwords.words('english')
text['content'] = text['content'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
text['content'].head()

### 2.4 Common word remove
freq = pd.Series(' '.join(text['content']).split()).value_counts()[:10]
freq

freq = list(freq.index)
text['content'] = text['content'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
text['content'].head()

### 2.5 Rare words removal

freq = pd.Series(' '.join(text['content']).split()).value_counts()[-10:]
freq

freq = list(freq.index)
text['content'] = text['content'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
text['content'].head()

import nltk
##nltk.download()

### 2.6 Spelling correction
### After install it at Anaconda Prompt, it works
### pip3 install textblob

from textblob import TextBlob
text['content'][:5].apply(lambda x: str(TextBlob(x).correct()))

### 2.7  Tokenization 

Toks = TextBlob(text['content'][1]).words
print(Toks)

### 2.8 Stemming

from nltk.stem import PorterStemmer
st = PorterStemmer()

### 2.9 Lemmatization

from textblob import Word
text['content'] = text['content'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))

text.head()


### 3.1 N-grams

TextBlob(text['content'][0]).ngrams(2)

### 3.2 Term frequncy
### TF = (Number of times term T appears in the particular row)/(number of terms in that row)

tf1 = (text['content'][1:2]).apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0).reset_index()
tf1.columns = ['words','tf']
tf1

### 3.5 Bag of words

from sklearn.feature_extraction.text import CountVectorizer

bow = CountVectorizer(max_features=1000, 
                      lowercase = True, 
                      ngram_range=(1,1),
                      analyzer   = "word")

train_bow = bow.fit_transform(text['content'])

train_bow

### 3.6 Sentiment Analysis

text['content'][:5].apply(lambda x: TextBlob(x).sentiment)
